{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8578589,"sourceType":"datasetVersion","datasetId":5130050}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"data = []\nwith open(\"/kaggle/input/image-captioning-for-visually-impaired/caption_eng/token-english.txt\",'r') as file:\n    for line in file:\n        parts = line.strip().split(\"\\t\")\n        if len(parts) == 2:\n            image_name_id , caption =  parts\n            image_name , id = image_name_id.split(\"#\")\n            data.append({\"image_name\":image_name,\"id\":id,\"caption\":caption})","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-30T18:21:58.155487Z","iopub.execute_input":"2024-11-30T18:21:58.155829Z","iopub.status.idle":"2024-11-30T18:21:58.175057Z","shell.execute_reply.started":"2024-11-30T18:21:58.155799Z","shell.execute_reply":"2024-11-30T18:21:58.174091Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"import pandas as pd\ndf = pd.DataFrame(data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T18:21:58.176891Z","iopub.execute_input":"2024-11-30T18:21:58.177138Z","iopub.status.idle":"2024-11-30T18:21:58.185948Z","shell.execute_reply.started":"2024-11-30T18:21:58.177114Z","shell.execute_reply":"2024-11-30T18:21:58.185268Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"data_2 = []\nwith open(\"/kaggle/input/image-captioning-for-visually-impaired/caption_eng/token-english2.txt\",'r') as file:\n    for line in file:\n        parts = line.strip().split(\"\\t\")\n        if len(parts) == 2:\n            image_name_id , caption =  parts\n            image_name , id = image_name_id.split(\"#\")\n            data.append({\"image_name\":image_name,\"id\":id,\"caption\":caption})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T18:21:58.186880Z","iopub.execute_input":"2024-11-30T18:21:58.187153Z","iopub.status.idle":"2024-11-30T18:21:58.205847Z","shell.execute_reply.started":"2024-11-30T18:21:58.187128Z","shell.execute_reply":"2024-11-30T18:21:58.205058Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"import pandas as pd\ndf_2 = pd.DataFrame(data_2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T18:21:58.207704Z","iopub.execute_input":"2024-11-30T18:21:58.207951Z","iopub.status.idle":"2024-11-30T18:21:58.212587Z","shell.execute_reply.started":"2024-11-30T18:21:58.207927Z","shell.execute_reply":"2024-11-30T18:21:58.211748Z"}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"data = pd.concat([df,df_2])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T18:21:58.213718Z","iopub.execute_input":"2024-11-30T18:21:58.214069Z","iopub.status.idle":"2024-11-30T18:21:58.223350Z","shell.execute_reply.started":"2024-11-30T18:21:58.214033Z","shell.execute_reply":"2024-11-30T18:21:58.222504Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"grouped_captions = data.groupby(\"image_name\")[\"caption\"].apply(list)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T18:21:58.224626Z","iopub.execute_input":"2024-11-30T18:21:58.224955Z","iopub.status.idle":"2024-11-30T18:21:58.281781Z","shell.execute_reply.started":"2024-11-30T18:21:58.224918Z","shell.execute_reply":"2024-11-30T18:21:58.281026Z"}},"outputs":[],"execution_count":42},{"cell_type":"code","source":"from transformers import ViTFeatureExtractor , AutoTokenizer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T18:21:58.283030Z","iopub.execute_input":"2024-11-30T18:21:58.283285Z","iopub.status.idle":"2024-11-30T18:21:58.286713Z","shell.execute_reply.started":"2024-11-30T18:21:58.283262Z","shell.execute_reply":"2024-11-30T18:21:58.285891Z"}},"outputs":[],"execution_count":43},{"cell_type":"code","source":"class config:\n    encoder = \"google/vit-base-patch16-224-in21k\"\n    decoder = \"gpt2\"\n    train_batch_size = 4","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T18:21:58.287703Z","iopub.execute_input":"2024-11-30T18:21:58.288015Z","iopub.status.idle":"2024-11-30T18:21:58.295528Z","shell.execute_reply.started":"2024-11-30T18:21:58.287968Z","shell.execute_reply":"2024-11-30T18:21:58.294719Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"def build_inputs_with_special_tokens(self,token_ids_0):\n    ouputs = [self.bos_token_id] + token_ids_0 + [self.eos_token_id]\n    return ouputs\nAutoTokenizer.build_inputs_with_special_tokens = build_inputs_with_special_tokens","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T18:21:58.296598Z","iopub.execute_input":"2024-11-30T18:21:58.297333Z","iopub.status.idle":"2024-11-30T18:21:58.307231Z","shell.execute_reply.started":"2024-11-30T18:21:58.297295Z","shell.execute_reply":"2024-11-30T18:21:58.305888Z"}},"outputs":[],"execution_count":45},{"cell_type":"code","source":"feature_extractor = ViTFeatureExtractor.from_pretrained(config.encoder)\ntokenizer = AutoTokenizer.from_pretrained(config.decoder)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T18:21:58.309825Z","iopub.execute_input":"2024-11-30T18:21:58.310383Z","iopub.status.idle":"2024-11-30T18:21:58.552717Z","shell.execute_reply.started":"2024-11-30T18:21:58.310356Z","shell.execute_reply":"2024-11-30T18:21:58.551746Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"}],"execution_count":46},{"cell_type":"code","source":"import os\nimport random\nimport torch\nfrom torch.utils.data import Dataset\nfrom PIL import Image\n\nclass ImageCaptioningDataset(Dataset):\n    def __init__(self, df, tokenizer, img_dir, feature_extractor, transform=None):\n        self.data = df\n        self.img_dir = img_dir\n        self.tokenizer = tokenizer\n        self.transform = transform\n        self.feature_extractor = feature_extractor\n        self.group_data = self.data.groupby(\"image_name\")[\"caption\"].apply(list)\n        self.max_length = 20\n    \n    def __len__(self):\n        return len(self.group_data)\n    \n    def __getitem__(self, idx):\n        img_name = self.group_data.index[idx]\n        captions = self.group_data.iloc[idx]\n        caption = random.choice(captions)\n        \n        self.tokenizer.pad_token = self.tokenizer.eos_token\n        \n        img_path = os.path.join(self.img_dir, img_name)\n        image = Image.open(img_path).convert(\"RGB\")\n        \n        if self.transform:\n            image = self.transform(image)\n        \n        pixels = self.feature_extractor(image, return_tensors=\"pt\").pixel_values\n        \n        tokenized_caption = self.tokenizer(\n            caption, \n            padding=\"max_length\", \n            truncation=True,\n            max_length=self.max_length, \n            return_tensors=\"pt\"\n        ).input_ids.squeeze(0)\n        \n        labels = tokenized_caption.clone()\n        labels[labels == self.tokenizer.pad_token_id] = -100\n        \n        return {\"pixel_values\": pixels.squeeze(), \"labels\": labels}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T18:21:58.554893Z","iopub.execute_input":"2024-11-30T18:21:58.555542Z","iopub.status.idle":"2024-11-30T18:21:58.563244Z","shell.execute_reply.started":"2024-11-30T18:21:58.555499Z","shell.execute_reply":"2024-11-30T18:21:58.562376Z"}},"outputs":[],"execution_count":47},{"cell_type":"code","source":"from torchvision import transforms\ntransforms = transforms.Compose([\n    transforms.Resize((224,224)),\n    transforms.ToTensor()\n])\nimg_dir = \"/kaggle/input/image-captioning-for-visually-impaired/images/images\"\nDATA = ImageCaptioningDataset(data,tokenizer,img_dir,feature_extractor=feature_extractor,transform=transforms)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T18:21:58.564503Z","iopub.execute_input":"2024-11-30T18:21:58.565343Z","iopub.status.idle":"2024-11-30T18:21:58.623584Z","shell.execute_reply.started":"2024-11-30T18:21:58.565289Z","shell.execute_reply":"2024-11-30T18:21:58.622976Z"}},"outputs":[],"execution_count":48},{"cell_type":"code","source":"import os ,random","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T18:21:58.625182Z","iopub.execute_input":"2024-11-30T18:21:58.625488Z","iopub.status.idle":"2024-11-30T18:21:58.629292Z","shell.execute_reply.started":"2024-11-30T18:21:58.625462Z","shell.execute_reply":"2024-11-30T18:21:58.628429Z"}},"outputs":[],"execution_count":49},{"cell_type":"code","source":"from torch.utils.data import random_split\n\n# Calculate the sizes for train and test splits\ntotal_size = len(DATA)\ntrain_size = int(0.8 * total_size)  # 80% for training\ntest_size = total_size - train_size  # Remaining 20% for testing\n\n# Split the dataset\ntrain_dataset, test_dataset = random_split(DATA, [train_size, test_size])\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T18:21:58.630359Z","iopub.execute_input":"2024-11-30T18:21:58.630623Z","iopub.status.idle":"2024-11-30T18:21:58.659797Z","shell.execute_reply.started":"2024-11-30T18:21:58.630598Z","shell.execute_reply":"2024-11-30T18:21:58.659177Z"}},"outputs":[],"execution_count":50},{"cell_type":"code","source":"from transformers import VisionEncoderDecoderModel \nmodel = VisionEncoderDecoderModel.from_encoder_decoder_pretrained(config.encoder,config.decoder)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T18:21:58.660761Z","iopub.execute_input":"2024-11-30T18:21:58.661094Z","iopub.status.idle":"2024-11-30T18:22:00.554955Z","shell.execute_reply.started":"2024-11-30T18:21:58.661055Z","shell.execute_reply":"2024-11-30T18:22:00.554007Z"}},"outputs":[{"name":"stderr","text":"Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.0.crossattention.c_attn.bias', 'h.0.crossattention.c_attn.weight', 'h.0.crossattention.c_proj.bias', 'h.0.crossattention.c_proj.weight', 'h.0.crossattention.q_attn.bias', 'h.0.crossattention.q_attn.weight', 'h.0.ln_cross_attn.bias', 'h.0.ln_cross_attn.weight', 'h.1.crossattention.c_attn.bias', 'h.1.crossattention.c_attn.weight', 'h.1.crossattention.c_proj.bias', 'h.1.crossattention.c_proj.weight', 'h.1.crossattention.q_attn.bias', 'h.1.crossattention.q_attn.weight', 'h.1.ln_cross_attn.bias', 'h.1.ln_cross_attn.weight', 'h.10.crossattention.c_attn.bias', 'h.10.crossattention.c_attn.weight', 'h.10.crossattention.c_proj.bias', 'h.10.crossattention.c_proj.weight', 'h.10.crossattention.q_attn.bias', 'h.10.crossattention.q_attn.weight', 'h.10.ln_cross_attn.bias', 'h.10.ln_cross_attn.weight', 'h.11.crossattention.c_attn.bias', 'h.11.crossattention.c_attn.weight', 'h.11.crossattention.c_proj.bias', 'h.11.crossattention.c_proj.weight', 'h.11.crossattention.q_attn.bias', 'h.11.crossattention.q_attn.weight', 'h.11.ln_cross_attn.bias', 'h.11.ln_cross_attn.weight', 'h.2.crossattention.c_attn.bias', 'h.2.crossattention.c_attn.weight', 'h.2.crossattention.c_proj.bias', 'h.2.crossattention.c_proj.weight', 'h.2.crossattention.q_attn.bias', 'h.2.crossattention.q_attn.weight', 'h.2.ln_cross_attn.bias', 'h.2.ln_cross_attn.weight', 'h.3.crossattention.c_attn.bias', 'h.3.crossattention.c_attn.weight', 'h.3.crossattention.c_proj.bias', 'h.3.crossattention.c_proj.weight', 'h.3.crossattention.q_attn.bias', 'h.3.crossattention.q_attn.weight', 'h.3.ln_cross_attn.bias', 'h.3.ln_cross_attn.weight', 'h.4.crossattention.c_attn.bias', 'h.4.crossattention.c_attn.weight', 'h.4.crossattention.c_proj.bias', 'h.4.crossattention.c_proj.weight', 'h.4.crossattention.q_attn.bias', 'h.4.crossattention.q_attn.weight', 'h.4.ln_cross_attn.bias', 'h.4.ln_cross_attn.weight', 'h.5.crossattention.c_attn.bias', 'h.5.crossattention.c_attn.weight', 'h.5.crossattention.c_proj.bias', 'h.5.crossattention.c_proj.weight', 'h.5.crossattention.q_attn.bias', 'h.5.crossattention.q_attn.weight', 'h.5.ln_cross_attn.bias', 'h.5.ln_cross_attn.weight', 'h.6.crossattention.c_attn.bias', 'h.6.crossattention.c_attn.weight', 'h.6.crossattention.c_proj.bias', 'h.6.crossattention.c_proj.weight', 'h.6.crossattention.q_attn.bias', 'h.6.crossattention.q_attn.weight', 'h.6.ln_cross_attn.bias', 'h.6.ln_cross_attn.weight', 'h.7.crossattention.c_attn.bias', 'h.7.crossattention.c_attn.weight', 'h.7.crossattention.c_proj.bias', 'h.7.crossattention.c_proj.weight', 'h.7.crossattention.q_attn.bias', 'h.7.crossattention.q_attn.weight', 'h.7.ln_cross_attn.bias', 'h.7.ln_cross_attn.weight', 'h.8.crossattention.c_attn.bias', 'h.8.crossattention.c_attn.weight', 'h.8.crossattention.c_proj.bias', 'h.8.crossattention.c_proj.weight', 'h.8.crossattention.q_attn.bias', 'h.8.crossattention.q_attn.weight', 'h.8.ln_cross_attn.bias', 'h.8.ln_cross_attn.weight', 'h.9.crossattention.c_attn.bias', 'h.9.crossattention.c_attn.weight', 'h.9.crossattention.c_proj.bias', 'h.9.crossattention.c_proj.weight', 'h.9.crossattention.q_attn.bias', 'h.9.crossattention.q_attn.weight', 'h.9.ln_cross_attn.bias', 'h.9.ln_cross_attn.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nVisionEncoderDecoderModel has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n","output_type":"stream"}],"execution_count":51},{"cell_type":"code","source":"from transformers import Seq2SeqTrainingArguments , Seq2SeqTrainer\n\ntrain_args = Seq2SeqTrainingArguments(per_device_train_batch_size=config.train_batch_size,eval_strategy=\"epoch\",\n                                     do_train=True,do_eval=True,num_train_epochs=5,output_dir=\"/kaggle/working/\",per_device_eval_batch_size=4,report_to=[\"none\"]  \n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T19:15:31.639038Z","iopub.execute_input":"2024-11-30T19:15:31.639768Z","iopub.status.idle":"2024-11-30T19:15:31.673854Z","shell.execute_reply.started":"2024-11-30T19:15:31.639730Z","shell.execute_reply":"2024-11-30T19:15:31.672869Z"}},"outputs":[],"execution_count":62},{"cell_type":"code","source":"model.config.decoder_start_token_id = tokenizer.bos_token_id\nmodel.config.pad_token_id = tokenizer.pad_token_id\nmodel.config.eos_token_id = tokenizer.sep_token_id\n\nmodel.config.vocab_size = 72\nmodel.config.max_lenght = 20\nmodel.config.early_stopping = True\nmodel.config.no_repeat_ngram_size = 3\nmodel.config.lenght_penalty = 2.0\nmodel.config.num_beams = 4\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T18:22:16.320751Z","iopub.execute_input":"2024-11-30T18:22:16.321399Z","iopub.status.idle":"2024-11-30T18:22:16.326430Z","shell.execute_reply.started":"2024-11-30T18:22:16.321359Z","shell.execute_reply":"2024-11-30T18:22:16.325427Z"}},"outputs":[],"execution_count":56},{"cell_type":"code","source":"import torch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T18:22:02.731803Z","iopub.execute_input":"2024-11-30T18:22:02.732069Z","iopub.status.idle":"2024-11-30T18:22:02.743691Z","shell.execute_reply.started":"2024-11-30T18:22:02.732044Z","shell.execute_reply":"2024-11-30T18:22:02.742864Z"}},"outputs":[],"execution_count":54},{"cell_type":"code","source":"trainer = Seq2SeqTrainer(model=model,tokenizer=feature_extractor,train_dataset=train_dataset,args=train_args,\n                         eval_dataset=test_dataset)\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T19:15:34.911589Z","iopub.execute_input":"2024-11-30T19:15:34.911924Z","iopub.status.idle":"2024-11-30T19:38:21.762723Z","shell.execute_reply.started":"2024-11-30T19:15:34.911897Z","shell.execute_reply":"2024-11-30T19:38:21.761796Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2790' max='2790' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2790/2790 22:45, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.360300</td>\n      <td>0.416860</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.353300</td>\n      <td>0.410268</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.349300</td>\n      <td>0.417573</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.345800</td>\n      <td>0.408083</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.351300</td>\n      <td>0.402623</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":63,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=2790, training_loss=0.3523199730876526, metrics={'train_runtime': 1366.2453, 'train_samples_per_second': 8.161, 'train_steps_per_second': 2.042, 'total_flos': 2.0121723859894272e+18, 'train_loss': 0.3523199730876526, 'epoch': 5.0})"},"metadata":{}}],"execution_count":63},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}